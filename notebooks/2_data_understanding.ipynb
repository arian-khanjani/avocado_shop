{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2 Data Understanding \n",
    "---\n",
    "\n",
    "In this phase, I will try to understand and explore the raw data in order to devise my approach towards how to clean and reformat it. I will use the following steps to understand the data:\n",
    "\n",
    "1. Data Collection and Setup\n",
    "2. Data Description\n",
    "3. Data Exploration\n",
    "4. Feature Analysis\n",
    "5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Collection & Setup\n",
    "---\n",
    "\n",
    "First, I need to load the data and the necessary libraries. After that, I will take a look at the data to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:39.121859Z",
     "start_time": "2025-03-09T16:04:39.094359Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from pylab import rcParams\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "avocados_df = pd.read_csv(\"../data/raw/avocado.csv\")\n",
    "avocados_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Description\n",
    "\n",
    "---\n",
    "\n",
    "In order to understand the data better, I must first analyse the data characteristics, each of its columns, and their properties.In this section, I will describe the dataset, covering its attributes, types, and any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:39.147870Z",
     "start_time": "2025-03-09T16:04:39.138443Z"
    }
   },
   "outputs": [],
   "source": [
    "avocados_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape of the dataset to understand the number of records and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:39.170733Z",
     "start_time": "2025-03-09T16:04:39.167880Z"
    }
   },
   "outputs": [],
   "source": [
    "print(avocados_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe each columns count, mean, std, min, 25%, 50%, 75%, max to gain a high level overview of each column. This can help spot any anomalies, missing values or extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:39.221115Z",
     "start_time": "2025-03-09T16:04:39.206941Z"
    }
   },
   "outputs": [],
   "source": [
    "avocados_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:39.311917Z",
     "start_time": "2025-03-09T16:04:39.308005Z"
    }
   },
   "outputs": [],
   "source": [
    "print(avocados_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicate entries using the built in method. This built in method might be unreliable as it might not detect all duplicate entries. Hence, I will check for other duplicates manually in later steps as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:39.380155Z",
     "start_time": "2025-03-09T16:04:39.371106Z"
    }
   },
   "outputs": [],
   "source": [
    "print(avocados_df.duplicated().sum(), \"duplicate entries found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the describe() menthod only shows the summary of the numerical columns, I will check the unique values of the categorical columns to understand them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:39.434070Z",
     "start_time": "2025-03-09T16:04:39.429795Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Unique types:', avocados_df['type'].unique())\n",
    "print('Number of unique regions:', avocados_df['region'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output and some help from the dataset documentation, I can understand the following about the dataset:\n",
    "\n",
    "**Dataset columns description:**\n",
    "\n",
    "- `Date` *(object)* - The date of the observation.\n",
    "- `AveragePrice` *(float64)* - The average price of a single avocado.\n",
    "- `TotalVolume` *(float64)* - Total number of avocados sold.\n",
    "- `4046` *(float64)* - Total number of avocados with PLU 4046 sold.\n",
    "- `4225` *(float64)* - Total number of avocados with PLU 4225 sold.\n",
    "- `4770` *(float64)* - Total number of avocados with PLU 4770 sold.\n",
    "- `TotalBags` *(float64)* - Total number of bags sold.\n",
    "- `SmallBags` *(float64)* - Total number of small bags sold.\n",
    "- `LargeBags` *(float64)* - Total number of large bags sold.\n",
    "- `XLargeBags` *(float64)* - Total number of extra-large bags sold.\n",
    "- `Year` *(int64)* - The year.\n",
    "- `Type` *(object)* - Conventional or organic.\n",
    "- `Region` *(object)* - The city or region of the observation.\n",
    "\n",
    "**Dataset information:**\n",
    "\n",
    "Based on the overview that we have seen so far, we can deduct the following general facts about the whole dataset for now:\n",
    "\n",
    "- **Count:** The total number of records in this dataset is 18,249\n",
    "- **Column [Unnamed 0]:** This column seems to be the index of each dataset as it resets when it reaches 51, hence this document must have been created by appending multiple datasets of 51 records together. This also means that there might be many duplicate or anomalous records in the dataset.\n",
    "- **Year:** Document description says that this started expanding at 2013, but the earliest record in it is from 2015. Also the last record is from 2018. We might only have data for a span of 3 years.\n",
    "- **Missing Values:** Interestingly, There are no missing values in this dataset. This could be a good sign, but we should still check for any placeholders or NaN values.\n",
    "- **Duplicate Values:** There are 0 duplicate entries in the dataset, indicating that all records are unique. This was checked using an automatic method in the code, hence we cannot be sure and need to investigate further.\n",
    "- **Types:** There are 3 types of data in this dataset: float64(9), int64(1), object(3)\n",
    "- **Regions:**: There are 54 unique regions in the dataset, but USA only has 50 states. This means that the dataset probably contains duplicate or invalid region values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, I will remove the 'Unnamed: 0' column as it is not necessary for the analysis. I will also change the name of all columns to lowercase and to use underscores instead of spaces for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:39.534294Z",
     "start_time": "2025-03-09T16:04:39.524954Z"
    }
   },
   "outputs": [],
   "source": [
    "avocados_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "avocados_df.columns = avocados_df.columns.str.lower().str.replace(' ', '_')\n",
    "avocados_df.rename(columns={'averageprice': 'average_price'}, inplace=True)\n",
    "avocados_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Exploration\n",
    "\n",
    "---\n",
    "\n",
    "Since the main goal is to predict the average price avocados in the next month, `average_price` column is the target variable. In this section, I will explore the data to understand the relationships between the features and the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brevity**\n",
    "\n",
    "In my later stages of analysis I found that one particular region, 'TotalUS', is most likely a sum of most of the other regions. This is a redundant data point and can be removed. I will remove this region from the dataset in an early stage for brevity, however I will try to justify my decesion with the following analysis. If I do not do this, the 'TotalUS' region will skew the results of the analysis and I have to repeat all of these steps again. \n",
    "\n",
    "First lets visualize the data to see the distribution of the regions and the total volume of avocados sold per region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:39.805168Z",
     "start_time": "2025-03-09T16:04:39.568467Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "avocados_df.groupby('region')['total_volume'].sum().plot(kind='bar', title=\"Total Volume of Avocados Sold by Region\")\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Total Volume')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TotalUS` region is clearly an outlier in terms of total volume sold. This is most likely because it is the sum of all regions. I will remove this region from the dataset and visualize the data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:40.009517Z",
     "start_time": "2025-03-09T16:04:39.823042Z"
    }
   },
   "outputs": [],
   "source": [
    "avocados_df_no_totalus = avocados_df.drop(avocados_df[avocados_df['region'] == 'TotalUS'].index)\n",
    "plt.figure(figsize=(20, 6))\n",
    "avocados_df_no_totalus.groupby('region')['total_volume'].sum().plot(kind='bar', title=\"Total Volume of Avocados Sold by Region\")\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Total Volume')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is extremely different now. The maximum total volume sold was 6m but now it is about 1.2m. This is a significant difference. However, I need to \n",
    "confirm my hypothesis. I will calculate the sum of total volume sold for all regions except 'TotalUS' and compare it with the total volume sold in 'TotalUS'. \n",
    "I will also try to do the same for other regions that might be aggregations as well.\n",
    "\n",
    "Other regions that might be aggregations:\n",
    "\n",
    "- `West`\n",
    "- `SouthCentral`\n",
    "- `SouthEast`\n",
    "- `NorthEast`\n",
    "- `Plains`\n",
    "- `GreatLakes`\n",
    "- `Midsouth`\n",
    "\n",
    "> Note: Total avocado sale of `California` is almost as big as `SouthCentral` and `West`. This might mean that California is also an aggreagation, however I will not include it in my aggregation list as it is a state and not a region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:40.022389Z",
     "start_time": "2025-03-09T16:04:40.017440Z"
    }
   },
   "outputs": [],
   "source": [
    "excluded_regions = [\n",
    "    'TotalUS', 'West', 'SouthCentral', 'SouthEast',\n",
    "    'NorthEast', 'Plains', 'GreatLakes', 'Midsouth'\n",
    "]\n",
    "\n",
    "total_df = avocados_df['total_volume'].sum()\n",
    "total_us_volume = avocados_df[avocados_df['region'] == 'TotalUS']['total_volume'].sum()\n",
    "total_no_excluded_regions = avocados_df[~avocados_df['region'].isin(excluded_regions)]['total_volume'].sum()\n",
    "total_only_excluded_regions = avocados_df[avocados_df['region'].isin(excluded_regions)]['total_volume'].sum()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "totals = {\n",
    "    'All Regions': total_df,\n",
    "    'TotalUS Only': total_us_volume,\n",
    "    'Non-Aggregated\\nRegions': total_no_excluded_regions,\n",
    "    'Aggregated\\nRegions': total_only_excluded_regions\n",
    "}\n",
    "\n",
    "totals_billions = {k: v/1e9 for k, v in totals.items()}\n",
    "plt.bar(totals.keys(), totals_billions.values())\n",
    "plt.title('Comparison of Total Sale Volumes for Aggregated and Non-Aggregated Regions')\n",
    "plt.ylabel('Total Volume (Billions)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f\"All regions (sum): {total_df:,.0f}\")\n",
    "print(f\"TotalUS only (sum): {total_us_volume:,.0f}\")\n",
    "print(f\"Non-aggregated regions (sum): {total_no_excluded_regions:,.0f}\")\n",
    "print(f\"Aggregated regions (sum): {total_only_excluded_regions:,.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other possible aggregations mentioned above summed together are not exactly the same as the `TotalUS` region. This can mean that other aggregations are not consistent throughout the data. Removing all these aggregations will destory the data and make it unusable.\n",
    "\n",
    "Hence, I will only remove the `TotalUS` region and keep the other aggregations for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocados_df = avocados_df.drop(avocados_df[avocados_df['region'] == 'TotalUS'].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outlier Detection**\n",
    "\n",
    "First, I will check the distribution of the target variable, `AveragePrice` based on each `type` of avocado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:40.105257Z",
     "start_time": "2025-03-09T16:04:40.041499Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='type', y='average_price', data=avocados_df)\n",
    "plt.title(\"Distribution of Average Price of Avocados by Type\")\n",
    "plt.xlabel(\"Type of Avocado\")\n",
    "plt.ylabel(\"Average Price\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot shows that the average price of organic avocados is higher than conventional avocados. It also highlights the presence of some outliers in the data. Hence, I must investigate the outliers further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print the number of records present for each type of avocado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:40.119913Z",
     "start_time": "2025-03-09T16:04:40.116521Z"
    }
   },
   "outputs": [],
   "source": [
    "print(avocados_df['type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both avocado types almost have the same number of records, which means the data is well distributed between the two types and no balancing is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:40.212831Z",
     "start_time": "2025-03-09T16:04:40.137764Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='type', y='total_volume', data=avocados_df)\n",
    "plt.title(\"Distribution of Total Volume of Avocados by Type\")\n",
    "plt.xlabel(\"Type of Avocado\")\n",
    "plt.ylabel(\"Total Volume\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This boxplot outlines some important points:\n",
    "    \n",
    "- Total Sold Volume of <em>Organic</em> avocados is much more consistent. This can be deducted as the number of records for each avocado type is almost the same.\n",
    "- The <em>Conventional</em> avocados probably have a lot of outliers, even though the number of records for each avocado type is relatively the same. This could be due to the presence of different regions in the dataset.\n",
    "\n",
    "I will now check the distribution of avocado PLU codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:40.363280Z",
     "start_time": "2025-03-09T16:04:40.224864Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=avocados_df[['4046', '4225', '4770']])\n",
    "plt.title(\"Distribution of Avocado PLU Codes\")\n",
    "plt.xlabel(\"PLU Codes\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.grid(True)\n",
    "plt.xticks([0, 1, 2], ['4046', '4225', '4770'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing these boxplot shows is that the data on codes `4046` and `4225` are very similar but the data on code `4770` is very different. This could be due to the fact that the data on `4770` is very different from the other two codes. It also outlines the presence of many outliers in the data with extreme values, as the difference is in millions (0-1m vs 6m)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Frequency**\n",
    "\n",
    "It is important to understand the frequency of the data in the dataset. Knowing the granularity of my data will help me to understand the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:40.453089Z",
     "start_time": "2025-03-09T16:04:40.368425Z"
    }
   },
   "outputs": [],
   "source": [
    "avocados_df['date'] = pd.to_datetime(avocados_df['date'])\n",
    "plt.figure(figsize=(20, 6))\n",
    "monthly_counts = avocados_df['date'].dt.to_period('M').value_counts().sort_index()\n",
    "monthly_counts.plot(kind='bar')\n",
    "plt.title(\"Number of Records in the Dataset per Month of Each Year\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Record Count\")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data frequency seems to be shifting between 425 and 530 between months and seems to be stable. Most likely no data balancing is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales volume trend by year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:40.518298Z",
     "start_time": "2025-03-09T16:04:40.459463Z"
    }
   },
   "outputs": [],
   "source": [
    "avocados_df.groupby(\"year\")['total_volume'].sum().plot(kind='bar', figsize=(20, 6))\n",
    "plt.title(\"Total Sales Volume of Avocados by Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Total Sales Volume\")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that either in 2018 the total sales has declined extremely or most likely the data for 2018 is incomplete. In order to make sure which is the case, I will check the total sale for each month in each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:40.635333Z",
     "start_time": "2025-03-09T16:04:40.529466Z"
    }
   },
   "outputs": [],
   "source": [
    "sales_per_year = avocados_df.copy()\n",
    "sales_per_year['date'] = pd.to_datetime(avocados_df['date'])\n",
    "sales_per_year['month'] = sales_per_year['date'].dt.month\n",
    "sales_per_year['year'] = sales_per_year['date'].dt.year\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.countplot(x='month', data=sales_per_year, hue='year')\n",
    "plt.title(\"Number of Sales by Month per Year\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of Sales\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Count Plot chart indicates that my hypothesis was correct and the count of records for 2018 is significantly lower than the other years. This is why the total sales for 2018 is lower than the other years.\n",
    "\n",
    "The lack of data in 2018 can affect the model's performance if I decide to predict the price over a year. However, as it is much better to increase the granulairty of the data scope by labeling the season or months and predict the price for e.g. each month, lack of data in 2018 most likely will not be a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Price Trends**\n",
    "\n",
    "In this section, I will investigate the price trends of avocados by type over the whole dataset to see how they change over the time and if there are seasonal patterns in the pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:41.693453Z",
     "start_time": "2025-03-09T16:04:40.640858Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.lineplot(x='date', y='average_price', hue='type', data=avocados_df)\n",
    "plt.axhline(y=avocados_df[avocados_df['type']=='conventional']['average_price'].mean(), \n",
    "           color='blue', linestyle='--', alpha=0.5, \n",
    "           label='Conventional Average')\n",
    "plt.axhline(y=avocados_df[avocados_df['type']=='organic']['average_price'].mean(),\n",
    "           color='orange', linestyle='--', alpha=0.5,\n",
    "           label='Organic Average') \n",
    "plt.title(\"Average Price Trends Over Time by Avocado Type\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Average Price\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line plot highlights some critical points as follows:\n",
    "\n",
    "- Even though the average price of organic avocados is higher than conventional avocados, they seem to share the same trend and seasonality. But this fact does not mean we can use one type instead of both or use a weighted average of the two types.\n",
    "\n",
    "- Prices are highest in 2017 and lowest in 2015, which suggests potential external factors not present in this dataset vastly affect the prices of avocados (such as poor harvest, weather, marketing, supply shortage or other economic factors...).\n",
    "\n",
    "- After each rise and fall, the prices seem to be returning their mean. This could suggest that the prices circle around a mean value and there could possibly be a seasonal pattern in the data.\n",
    "\n",
    "However, I will have to explore further in later steps to understand the seasonality in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Feature Analysis\n",
    "\n",
    "---\n",
    "\n",
    "#### Pearson Correlation Matrix\n",
    "\n",
    "First I will visualize the Pearson correlation matrix to understand the relationships between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:41.938488Z",
     "start_time": "2025-03-09T16:04:41.740075Z"
    }
   },
   "outputs": [],
   "source": [
    "avocados_df_corr = avocados_df.drop(['date', 'type', 'region'], axis=1)\n",
    "\n",
    "corr_matrix = avocados_df_corr.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix of Avocado Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Heatmap, there are not much meaningful correlations. This could suggest that the current form of our data is not ready to be analyzed. However, we could deduct the following:\n",
    "\n",
    "**`Total Volume` and `PLU 4046`, `PLU 4225`:**\n",
    "\n",
    "- This strong positive correlation could suggest that the total volume of avocados sold is highly correlated with the total number of avocados sold with PLU 4046, then 4224 and 4770, respectively. This might mean that the majority of sales records are for PLU 4046 and 4225 avocados.\n",
    "\n",
    "**`Total Volume` and `Bags`:**\n",
    "\n",
    "- This strong positive correlation could suggest that the total volume of avocados sold is highly correlated with the total number of bags sold. \n",
    "\n",
    "**`Total Volume` and `Average Price`:**\n",
    "\n",
    "- This weak negative correlation could suggest that total volume of avocados sold increases as the average price of avocados decreases or vice versa.\n",
    "\n",
    "For now, I can mark some features to be important for further analysis and modeling:\n",
    "\n",
    "- `Total Volume`\n",
    "- `Average Price`\n",
    "- `PLU codes`\n",
    "- `Total Bags`\n",
    "\n",
    "As these features have strong positive correlation with the `Total Volume` and a weak negative correlation with the `Average Price`, I will focus on these features in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bags**\n",
    "\n",
    "Since bags can be potential features and `total_bags` seems to be the sum of all other bag size sales and the correlation of different bag sizes are almost the same as their total, it is better to discard the redundant columns. \n",
    "\n",
    "I will first prove my hypothesis and upon vlidation, remove all other fields for bag sizes and only keep the total bags sale as others do not produce much value into my analysis.\n",
    "\n",
    "Verify if the `total_bags` column is the sum of the other 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Bags Sales Sum:\", avocados_df['total_bags'].sum())\n",
    "print(\"Bag Size Sales Sum:  \", avocados_df['small_bags'].sum() + avocados_df['large_bags'].sum() + avocados_df['xlarge_bags'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only 10 sales difference between the `total_bags` column and the sum of the other 3 columns. This difference is negligible and can be ignored. Therefore, I will drop the other 3 columns and keep the `total_bags` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocados_df = avocados_df.drop(columns=['small_bags', 'large_bags', 'xlarge_bags'])\n",
    "avocados_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Total Volume` and `Average Price`**\n",
    "\n",
    "The natural relationship of these two fields in almost every business is to be inverse. As the correlation betweem them is negative (hence, natural), I will leave them as they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Year`**\n",
    "\n",
    "The `year` field seems to be almost neutral in correelation with other fields. Slightly positive for some fields, and slightly negative for others. This might suggest that `year` is not a good feature to use for modeling. However, I will keep it as it is for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is crucial to explore and discover more features that could be more useful for the analysis, I will perform another correlation analysis after the data preparation step, to see if I can discover more features that could be more useful for the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regions\n",
    "\n",
    "In order to get a better understanding of regions, first I will quickly check all the available region data and identify any inconsistencies or missing information. This will help in validating the regions better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:41.972240Z",
     "start_time": "2025-03-09T16:04:41.964145Z"
    }
   },
   "outputs": [],
   "source": [
    "region_count = avocados_df.groupby('region')['date'].count().sort_values()\n",
    "region_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the histogram [from the **Brevity** section], we saw some anomalies in regions. Values such as (`TotalUS`, `Plains`, `West`, ...) are not states or cities. There seem to be aggregations, duplicates or wrong values that need to be addressed.\n",
    "\n",
    "In order to use the regions for modeling, I will need to clean them by aggregating them into more meaningful regions. As this can be a time consuming task, I will ask ChatGPT to analyze and group the regions into bigger <em>areas</em>. Doing this task manually is time consuming and error prone. \n",
    "\n",
    "The following is the result of ChatGPTs analysis:\n",
    "\n",
    "**Anomalies**\n",
    "\n",
    "- **TotalUS:** This is not a region, but a sum of all regions.\n",
    "\n",
    "And the following regions cover multiple states:\n",
    "\n",
    "- **Plains:** Kansas, Nebraska, etc...\n",
    "- **West:** California, Washington, etc...\n",
    "- **SouthCentral:** Texas, Oklahoma, etc...\n",
    "- **GreatLakes:** Illinois, Michigan, etc...\n",
    "- **Midsouth:** Mississippi, Arkansas, etc...\n",
    "- **Northeast:** New York, Pennsylvania, etc...\n",
    "- **Southeast:** Florida, Georgia, etc...\n",
    "\n",
    "**Aggregation**\n",
    "\n",
    "The regions can be grouped into the following areas:\n",
    "\n",
    "1. **Northeast:** New York, Philadelphia, Boston, BaltimoreWashington, HartfordSpringfield, Buffalo-Rochester, Pittsburgh, Syracuse, Harrisburg-Scranton, Albany, Northern New England.\n",
    "2. **Midwest:** Chicago, Detroit, Columbus, CincinnatiDayton, St. Louis, Grand Rapids, Indianapolis, Great Lakes, Plains.\n",
    "3. **South:** Atlanta, Charlotte, MiamiFtLauderdale, DallasFtWorth, Houston, Nashville, RaleighGreensboro, Tampa, Orlando, Jacksonville, NewOrleansMobile, Louisville, RichmondNorfolk, SouthCarolina, SouthCentral, Midsouth, Southeast, Roanoke.\n",
    "4. **West:** LosAngeles, SanFrancisco, SanDiego, Seattle, Portland, Denver, LasVegas, Sacramento, Spokane, Boise, PhoenixTucson, West, WestTexNewMexico, California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:42.039263Z",
     "start_time": "2025-03-09T16:04:42.028503Z"
    }
   },
   "outputs": [],
   "source": [
    "northeast = [\n",
    "    \"Albany\", \"BaltimoreWashington\", \"Boston\", \"BuffaloRochester\", \"HarrisburgScranton\",\n",
    "    \"HartfordSpringfield\", \"NewYork\", \"Philadelphia\", \"Pittsburgh\", \"Syracuse\",\n",
    "    \"Northeast\", \"NorthernNewEngland\"\n",
    "]\n",
    "\n",
    "midwest = [\n",
    "    \"Chicago\", \"CincinnatiDayton\", \"Columbus\", \"Detroit\", \"GrandRapids\",\n",
    "    \"GreatLakes\", \"Indianapolis\", \"Plains\", \"StLouis\"\n",
    "]\n",
    "\n",
    "south = [\n",
    "    \"Atlanta\", \"Charlotte\", \"DallasFtWorth\", \"Houston\", \"Jacksonville\",\n",
    "    \"Louisville\", \"MiamiFtLauderdale\", \"Midsouth\", \"Nashville\",\n",
    "    \"NewOrleansMobile\", \"Orlando\", \"RaleighGreensboro\", \"RichmondNorfolk\",\n",
    "    \"SouthCarolina\", \"SouthCentral\", \"Southeast\", \"Tampa\", \"Roanoke\"\n",
    "]\n",
    "\n",
    "west = [\n",
    "    \"Boise\", \"California\", \"Denver\", \"LasVegas\", \"LosAngeles\", \"PhoenixTucson\",\n",
    "    \"Portland\", \"Sacramento\", \"SanDiego\", \"SanFrancisco\", \"Seattle\", \"Spokane\",\n",
    "    \"West\", \"WestTexNewMexico\"\n",
    "]\n",
    "\n",
    "region_to_area = {region: \"Northeast\" for region in northeast}\n",
    "region_to_area.update({region: \"Midwest\" for region in midwest})\n",
    "region_to_area.update({region: \"South\" for region in south})\n",
    "region_to_area.update({region: \"West\" for region in west})\n",
    "\n",
    "avocados_df[\"area\"] = avocados_df[\"region\"].map(region_to_area).fillna(\"Unknown\")\n",
    "\n",
    "print(\"Number of 'Unknown' imputations: \", avocados_df[avocados_df['area'] == 'Unknown'].shape[0])\n",
    "avocados_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will visualize the data again to see if the area aggregation was successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:42.280051Z",
     "start_time": "2025-03-09T16:04:42.168797Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.boxplot(data=avocados_df, x='area', y='average_price')\n",
    "plt.title('Distribution of Average Price of Avocados by Area')\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Average Price')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is true that we have lost our granularity in `region` and `area`, however, this will help us understand the data better and make more accurate predictions as the regions values contained too many duplicated, vague, ambigous values that were practically useless if used them in their current state.\n",
    "\n",
    "It seems that the new `area` feature does not provide much valuable information regarding the effect of region on average price. However, it is also not completely useless. I will keep it for now to see if the model can utilize its values after enconding.\n",
    "\n",
    "Also, it seems like the <em>Northeast</em> region has the most expensive avocados in US!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Redundant Column**\n",
    "\n",
    "Now that I have introduced the `area` column, I believe that the `region` column is redundant and can be removed. I will remove this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:42.346446Z",
     "start_time": "2025-03-09T16:04:42.332878Z"
    }
   },
   "outputs": [],
   "source": [
    "avocados_df = avocados_df.drop(columns=['region'])\n",
    "avocados_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seasonality\n",
    "\n",
    "Inorder to introduce seasonal granularity to the dataset, I will add a new column called `season` and `month_name`. Afterwards, I will aggregate the regions to their respective areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Months**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:42.479998Z",
     "start_time": "2025-03-09T16:04:42.425238Z"
    }
   },
   "outputs": [],
   "source": [
    "avocados_df['month_name'] = avocados_df['date'].dt.strftime('%B')\n",
    "avocados_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the distribution of the data based on the month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:42.784870Z",
     "start_time": "2025-03-09T16:04:42.620693Z"
    }
   },
   "outputs": [],
   "source": [
    "avocados_df = avocados_df.sort_values('date')\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.lineplot(x='month_name', y='total_volume', hue='year', data=avocados_df, errorbar=None)\n",
    "plt.title(\"Total Volume of Avocados Sold by Month per Year\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Total Volume\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line chart can suggest the possibility of cyclical patterns in the data. The data seems to be more consistent in the summer months, which could suggest that the prices of avocados are more stable in the summer months. However, possibly the data may not contain trends or seasonality as the patterns do not seem to be in fixed intervals, furthermore, there are only one or two trends (peaks) in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seasons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:42.834574Z",
     "start_time": "2025-03-09T16:04:42.825200Z"
    }
   },
   "outputs": [],
   "source": [
    "seasons = {\n",
    "    1: 'Winter',\n",
    "    2: 'Winter',\n",
    "    3: 'Spring',\n",
    "    4: 'Spring',\n",
    "    5: 'Spring',\n",
    "    6: 'Summer',\n",
    "    7: 'Summer',\n",
    "    8: 'Summer',\n",
    "    9: 'Autumn',\n",
    "    10: 'Autumn',\n",
    "    11: 'Autumn',\n",
    "    12: 'Winter'\n",
    "}\n",
    "avocados_df['season'] = avocados_df['date'].dt.month.map(seasons)\n",
    "\n",
    "avocados_df['season'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the distribution of the data based on the season:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='season', y='average_price', data=avocados_df)\n",
    "plt.title(\"Distribution of Average Price of Avocados Sold by Season\")\n",
    "plt.xlabel(\"Season\")\n",
    "plt.ylabel(\"Average Price\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:43.154572Z",
     "start_time": "2025-03-09T16:04:43.007429Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.lineplot(x='season', y='total_volume', hue='year', data=avocados_df, errorbar=None)\n",
    "plt.title(\"Total Volume of Avocados Sold by Season per Year\")\n",
    "plt.xlabel(\"Season\")\n",
    "plt.ylabel(\"Total Volume\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart is very interesting as it outlines very key trends in the data that can help us understand customer behavior better. \n",
    "\n",
    "- It seems like the data on 2018 might be entirely misleading and might be worth removing it.\n",
    "- During the other years, Fall season has had the least sale volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validate**\n",
    "\n",
    "Check to see if new entires do not have any missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:43.181392Z",
     "start_time": "2025-03-09T16:04:43.176807Z"
    }
   },
   "outputs": [],
   "source": [
    "print(avocados_df[['month_name', 'season']].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "Adding seasons does not add much value more than the months. Also, as the goal is to predict the price of avocados by month and not season, I will not use them in later steps and will only use the months column that I have added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "avocados_df = avocados_df.drop(columns=['season'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seasonal Decomposition**\n",
    "\n",
    "In order to understand the seasonality in the dataset better, I will decompose the data into its seasonal, trend, and residual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 18, 8\n",
    "s_decompotions = avocados_df.copy()\n",
    "s_decompotions[\"date\"] = pd.to_datetime(s_decompotions[\"date\"])\n",
    "s_decompotions = s_decompotions.sort_values(by=\"date\")\n",
    "s_decompotions.set_index(\"date\", inplace=True)\n",
    "y = s_decompotions[\"average_price\"]\n",
    "y = y.resample(\"W\").mean()\n",
    "\n",
    "decomposition = sm.tsa.seasonal_decompose(y, model='additive', period=12)  # 12: Monthly seasonality\n",
    "\n",
    "fig = decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trend: \n",
    "\n",
    "- The trend component of the data seems to be decreasing at first and increasing a few times over time. However, the trend is not very strong and also not linear.\n",
    "\n",
    "Seasonal:\n",
    "\n",
    "- The price of avocado is clear to be fluctuating seasonally.\n",
    "\n",
    "Residual (random noise):\n",
    "\n",
    "- It is hard to say of there is a pattern in the residual component. If there is a pattern, other factors are influencing the price of avocados. If not, the model will probably fit well. My personal opinion is that there is almost a pattern in the residual component.\n",
    "\n",
    "The previous assumptions can help with choosing the right model for the job. The seasonality suggests utilizing a model such as SARIMA or Prophet is a good idea. If the trend is clearly linear, it can be a good idea to use XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stationarity**\n",
    "\n",
    "It is important to check the stationarity of the data incase I decide to use certain statistical models such as SARIMA. In order to check if the data is stationary, I will use the Augmented Dickey-Fuller test. I will check the stationarity of the data based on the `Average Price` of avocados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adfuller(avocados_df[\"average_price\"])\n",
    "\n",
    "print(\"ADF Statistic:\", result[0])\n",
    "print(\"p-value:        {:.14f}\".format(result[1]))\n",
    "\n",
    "if result[1] > 0.05:\n",
    "    print(\"The data is non-stationary (p > 0.05)\")\n",
    "else:\n",
    "    print(\"The data is stationary (p ≤ 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the reult, the data is stationary as p-value is lower than 0.05. Is this value is correct, it means that I can skip differencing the data and use the data directly for modeling using either ARIAMA, SARIMA or Prophet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Output**\n",
    "\n",
    "Save the output to a new CSV file for the next step (3 Data Preparation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:04:43.496006Z",
     "start_time": "2025-03-09T16:04:43.320103Z"
    }
   },
   "outputs": [],
   "source": [
    "avocados_df = avocados_df.sort_index(axis=1)\n",
    "avocados_df.to_csv(\"../data/interim/avocado_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Conclusion\n",
    "\n",
    "---\n",
    "\n",
    "So far I have analyzed the data as much as I could and have made the following changes:\n",
    "\n",
    "- Removed the `Unnamed: 0` column as it was not necessary.\n",
    "- Grouped the regions into more meaningful `areas`.\n",
    "- Added new `month_name` and `season` columns, but removed the `season` as it did not add much value more than `month_name`.\n",
    "- Removed the `small_bags`, `large_bags`, `xlarge_bags`, `region`, `total_bags` and `Unnamed: 0` columns.\n",
    "- Visualized the data to understand the distribution of the data better.\n",
    "- Lowercased the column names for consistency.\n",
    "- Correlation matrix analysis to understand the relationships between the features.\n",
    "- Visualized the data seasonal decomposition to understand the seasonality of the data.\n",
    "- Checked the stationarity of the data.\n",
    "- Saved the output to a new CSV file for the next step (3 Data Preparation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
